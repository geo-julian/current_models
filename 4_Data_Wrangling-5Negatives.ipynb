{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Data Wrangling\n",
    "\n",
    "The raw output data from coregistration step cannot be passed into machine learning model directly. They don't have labels and have too many features. \n",
    "\n",
    "The label is the thing we will try to predict with the machine learning models. In our case, the label is a True or False assertion. Each input data row is either a mineral deposit or not a mineral deposit. Given a data row, the machine learning model will tell us if the location is a mineral deposit or not. This is the ultimate goal we are trying to achieve in this machine learning workflow.\n",
    "\n",
    "The feature is a column of the input data. There are many features in the coregistration output data. It is not wise to use too many features in the machine learning analysis because of the [Curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality). More features mean more dimensions. If you have 20 features/columns, the machine learning model has to do the analysis in a 20-dimensional space. Just imagine if you were thrown into a 20-dimensional space, I guess it would not be a happy experience to find a way out. Yes, some machine learning models are smart enough and can reduce the number of dimensions. But at the current point of human history, humans are still a little bit smarter than computers. So, let's help computers by reducing the number of features.\n",
    "\n",
    "We will create a csv file, in which the last colomn is the label(0 or 1) and the other columns are features. \n",
    "\n",
    "The feature selection is highly related to the specific research. We need to identify the features which are most important to the formation of mineral deposit. For example, some researchers might think the distance along the trench is important. Others might think the sea floor age is of great significance. Come out with your own hypothesis, wrangle the data accordingly and then send the data into a machine learning model to be evaluated. Repeat this process until we find the most important features. This process is similar to a psychic finding a perfect crystal ball to start a fortune telling business. The only difference is that the psychic is doing magic, but we are doing science.\n",
    "\n",
    "The following code cell will select features from coregistration output and create a csv file for the machine learning analysis in Step 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of coregistration input data is:  (272, 5)\n",
      "The shape of coregistration output data is:  (272, 28)\n",
      "Good! The input and output data has the same length  272\n",
      "\n",
      "the coregistration input data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>age</th>\n",
       "      <th>plate_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-157.24</td>\n",
       "      <td>57.05</td>\n",
       "      <td>0</td>\n",
       "      <td>16112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-158.40</td>\n",
       "      <td>56.52</td>\n",
       "      <td>4</td>\n",
       "      <td>16112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-157.24</td>\n",
       "      <td>57.05</td>\n",
       "      <td>5</td>\n",
       "      <td>16112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-127.85</td>\n",
       "      <td>50.68</td>\n",
       "      <td>5</td>\n",
       "      <td>16110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-127.58</td>\n",
       "      <td>50.33</td>\n",
       "      <td>5</td>\n",
       "      <td>16110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>267</td>\n",
       "      <td>-127.39</td>\n",
       "      <td>50.59</td>\n",
       "      <td>167</td>\n",
       "      <td>16110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>268</td>\n",
       "      <td>-127.46</td>\n",
       "      <td>50.59</td>\n",
       "      <td>167</td>\n",
       "      <td>16110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>269</td>\n",
       "      <td>-127.86</td>\n",
       "      <td>50.67</td>\n",
       "      <td>168</td>\n",
       "      <td>16110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>270</td>\n",
       "      <td>-120.00</td>\n",
       "      <td>49.17</td>\n",
       "      <td>168</td>\n",
       "      <td>16102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>271</td>\n",
       "      <td>-119.27</td>\n",
       "      <td>38.96</td>\n",
       "      <td>169</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>272 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index     lon    lat  age  plate_id\n",
       "0        0 -157.24  57.05    0     16112\n",
       "1        1 -158.40  56.52    4     16112\n",
       "2        2 -157.24  57.05    5     16112\n",
       "3        3 -127.85  50.68    5     16110\n",
       "4        4 -127.58  50.33    5     16110\n",
       "..     ...     ...    ...  ...       ...\n",
       "267    267 -127.39  50.59  167     16110\n",
       "268    268 -127.46  50.59  167     16110\n",
       "269    269 -127.86  50.67  168     16110\n",
       "270    270 -120.00  49.17  168     16102\n",
       "271    271 -119.27  38.96  169       178\n",
       "\n",
       "[272 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the coregistration output data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>age</th>\n",
       "      <th>plate_id</th>\n",
       "      <th>recon_lon</th>\n",
       "      <th>recon_lat</th>\n",
       "      <th>distance</th>\n",
       "      <th>sub_idx</th>\n",
       "      <th>trench_lon</th>\n",
       "      <th>trench_lat</th>\n",
       "      <th>...</th>\n",
       "      <th>dist_nearest_edge</th>\n",
       "      <th>dist_from_start</th>\n",
       "      <th>conv_ortho</th>\n",
       "      <th>conv_paral</th>\n",
       "      <th>trench_abs_ortho</th>\n",
       "      <th>trench_abs_paral</th>\n",
       "      <th>subducting_abs_rate</th>\n",
       "      <th>subducting_abs_angle</th>\n",
       "      <th>subducting_abs_ortho</th>\n",
       "      <th>subducting_abs_paral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-157.24</td>\n",
       "      <td>57.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16112.0</td>\n",
       "      <td>-157.240</td>\n",
       "      <td>57.050</td>\n",
       "      <td>0.034</td>\n",
       "      <td>255.0</td>\n",
       "      <td>-154.67</td>\n",
       "      <td>55.75</td>\n",
       "      <td>...</td>\n",
       "      <td>7.25</td>\n",
       "      <td>7.25</td>\n",
       "      <td>5.40</td>\n",
       "      <td>1.10</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>-4.77</td>\n",
       "      <td>-10.90</td>\n",
       "      <td>4.69</td>\n",
       "      <td>-0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-158.40</td>\n",
       "      <td>56.52</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16112.0</td>\n",
       "      <td>-157.842</td>\n",
       "      <td>57.224</td>\n",
       "      <td>0.033</td>\n",
       "      <td>166.0</td>\n",
       "      <td>-155.99</td>\n",
       "      <td>55.66</td>\n",
       "      <td>...</td>\n",
       "      <td>8.60</td>\n",
       "      <td>8.60</td>\n",
       "      <td>5.56</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>-1.85</td>\n",
       "      <td>-4.81</td>\n",
       "      <td>-20.12</td>\n",
       "      <td>4.51</td>\n",
       "      <td>-1.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-157.24</td>\n",
       "      <td>57.05</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16112.0</td>\n",
       "      <td>-156.497</td>\n",
       "      <td>57.921</td>\n",
       "      <td>0.034</td>\n",
       "      <td>235.0</td>\n",
       "      <td>-153.90</td>\n",
       "      <td>56.61</td>\n",
       "      <td>...</td>\n",
       "      <td>7.25</td>\n",
       "      <td>7.25</td>\n",
       "      <td>5.27</td>\n",
       "      <td>1.21</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>-2.05</td>\n",
       "      <td>-4.67</td>\n",
       "      <td>-10.38</td>\n",
       "      <td>4.59</td>\n",
       "      <td>-0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-127.85</td>\n",
       "      <td>50.68</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16110.0</td>\n",
       "      <td>-126.765</td>\n",
       "      <td>51.250</td>\n",
       "      <td>0.018</td>\n",
       "      <td>291.0</td>\n",
       "      <td>-127.88</td>\n",
       "      <td>50.52</td>\n",
       "      <td>...</td>\n",
       "      <td>1.68</td>\n",
       "      <td>11.38</td>\n",
       "      <td>3.38</td>\n",
       "      <td>1.83</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>-2.45</td>\n",
       "      <td>23.93</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-127.58</td>\n",
       "      <td>50.33</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16110.0</td>\n",
       "      <td>-126.504</td>\n",
       "      <td>50.896</td>\n",
       "      <td>0.016</td>\n",
       "      <td>293.0</td>\n",
       "      <td>-127.57</td>\n",
       "      <td>50.32</td>\n",
       "      <td>...</td>\n",
       "      <td>1.96</td>\n",
       "      <td>11.09</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.43</td>\n",
       "      <td>-1.26</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>-2.45</td>\n",
       "      <td>17.34</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>-127.39</td>\n",
       "      <td>50.59</td>\n",
       "      <td>167.0</td>\n",
       "      <td>16110.0</td>\n",
       "      <td>-54.951</td>\n",
       "      <td>37.991</td>\n",
       "      <td>0.128</td>\n",
       "      <td>1470.0</td>\n",
       "      <td>-63.36</td>\n",
       "      <td>41.44</td>\n",
       "      <td>...</td>\n",
       "      <td>7.27</td>\n",
       "      <td>7.27</td>\n",
       "      <td>4.95</td>\n",
       "      <td>-4.05</td>\n",
       "      <td>-1.52</td>\n",
       "      <td>2.33</td>\n",
       "      <td>-3.83</td>\n",
       "      <td>-26.68</td>\n",
       "      <td>3.42</td>\n",
       "      <td>-1.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>-127.46</td>\n",
       "      <td>50.59</td>\n",
       "      <td>167.0</td>\n",
       "      <td>16110.0</td>\n",
       "      <td>-54.994</td>\n",
       "      <td>38.019</td>\n",
       "      <td>0.127</td>\n",
       "      <td>1470.0</td>\n",
       "      <td>-63.36</td>\n",
       "      <td>41.44</td>\n",
       "      <td>...</td>\n",
       "      <td>7.27</td>\n",
       "      <td>7.27</td>\n",
       "      <td>4.95</td>\n",
       "      <td>-4.05</td>\n",
       "      <td>-1.52</td>\n",
       "      <td>2.33</td>\n",
       "      <td>-3.83</td>\n",
       "      <td>-26.68</td>\n",
       "      <td>3.42</td>\n",
       "      <td>-1.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>-127.86</td>\n",
       "      <td>50.67</td>\n",
       "      <td>168.0</td>\n",
       "      <td>16110.0</td>\n",
       "      <td>-54.800</td>\n",
       "      <td>38.273</td>\n",
       "      <td>0.129</td>\n",
       "      <td>1468.0</td>\n",
       "      <td>-63.63</td>\n",
       "      <td>41.29</td>\n",
       "      <td>...</td>\n",
       "      <td>7.54</td>\n",
       "      <td>7.54</td>\n",
       "      <td>4.96</td>\n",
       "      <td>-4.02</td>\n",
       "      <td>-1.52</td>\n",
       "      <td>2.33</td>\n",
       "      <td>-3.83</td>\n",
       "      <td>-26.32</td>\n",
       "      <td>3.43</td>\n",
       "      <td>-1.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>-120.00</td>\n",
       "      <td>49.17</td>\n",
       "      <td>168.0</td>\n",
       "      <td>16102.0</td>\n",
       "      <td>-51.241</td>\n",
       "      <td>31.511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>-119.27</td>\n",
       "      <td>38.96</td>\n",
       "      <td>169.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>-56.884</td>\n",
       "      <td>22.713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>272 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lon    lat    age  plate_id  recon_lon  recon_lat  distance  sub_idx  \\\n",
       "0   -157.24  57.05    0.0   16112.0   -157.240     57.050     0.034    255.0   \n",
       "1   -158.40  56.52    4.0   16112.0   -157.842     57.224     0.033    166.0   \n",
       "2   -157.24  57.05    5.0   16112.0   -156.497     57.921     0.034    235.0   \n",
       "3   -127.85  50.68    5.0   16110.0   -126.765     51.250     0.018    291.0   \n",
       "4   -127.58  50.33    5.0   16110.0   -126.504     50.896     0.016    293.0   \n",
       "..      ...    ...    ...       ...        ...        ...       ...      ...   \n",
       "267 -127.39  50.59  167.0   16110.0    -54.951     37.991     0.128   1470.0   \n",
       "268 -127.46  50.59  167.0   16110.0    -54.994     38.019     0.127   1470.0   \n",
       "269 -127.86  50.67  168.0   16110.0    -54.800     38.273     0.129   1468.0   \n",
       "270 -120.00  49.17  168.0   16102.0    -51.241     31.511       NaN      NaN   \n",
       "271 -119.27  38.96  169.0     178.0    -56.884     22.713       NaN      NaN   \n",
       "\n",
       "     trench_lon  trench_lat  ...  dist_nearest_edge  dist_from_start  \\\n",
       "0       -154.67       55.75  ...               7.25             7.25   \n",
       "1       -155.99       55.66  ...               8.60             8.60   \n",
       "2       -153.90       56.61  ...               7.25             7.25   \n",
       "3       -127.88       50.52  ...               1.68            11.38   \n",
       "4       -127.57       50.32  ...               1.96            11.09   \n",
       "..          ...         ...  ...                ...              ...   \n",
       "267      -63.36       41.44  ...               7.27             7.27   \n",
       "268      -63.36       41.44  ...               7.27             7.27   \n",
       "269      -63.63       41.29  ...               7.54             7.54   \n",
       "270         NaN         NaN  ...                NaN              NaN   \n",
       "271         NaN         NaN  ...                NaN              NaN   \n",
       "\n",
       "     conv_ortho  conv_paral  trench_abs_ortho  trench_abs_paral  \\\n",
       "0          5.40        1.10             -0.72             -2.00   \n",
       "1          5.56        0.20             -1.05             -1.85   \n",
       "2          5.27        1.21             -0.69             -2.05   \n",
       "3          3.38        1.83             -1.15             -0.82   \n",
       "4          3.60        1.43             -1.26             -0.69   \n",
       "..          ...         ...               ...               ...   \n",
       "267        4.95       -4.05             -1.52              2.33   \n",
       "268        4.95       -4.05             -1.52              2.33   \n",
       "269        4.96       -4.02             -1.52              2.33   \n",
       "270         NaN         NaN               NaN               NaN   \n",
       "271         NaN         NaN               NaN               NaN   \n",
       "\n",
       "     subducting_abs_rate  subducting_abs_angle  subducting_abs_ortho  \\\n",
       "0                  -4.77                -10.90                  4.69   \n",
       "1                  -4.81                -20.12                  4.51   \n",
       "2                  -4.67                -10.38                  4.59   \n",
       "3                  -2.45                 23.93                  2.24   \n",
       "4                  -2.45                 17.34                  2.34   \n",
       "..                   ...                   ...                   ...   \n",
       "267                -3.83                -26.68                  3.42   \n",
       "268                -3.83                -26.68                  3.42   \n",
       "269                -3.83                -26.32                  3.43   \n",
       "270                  NaN                   NaN                   NaN   \n",
       "271                  NaN                   NaN                   NaN   \n",
       "\n",
       "     subducting_abs_paral  \n",
       "0                   -0.90  \n",
       "1                   -1.65  \n",
       "2                   -0.84  \n",
       "3                    0.99  \n",
       "4                    0.73  \n",
       "..                    ...  \n",
       "267                 -1.72  \n",
       "268                 -1.72  \n",
       "269                 -1.70  \n",
       "270                   NaN  \n",
       "271                   NaN  \n",
       "\n",
       "[272 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the columns in coregistration output data are: \n",
      "* 0 reconstructed mineral deposits longitude\n",
      "* 1 reconstructed mineral deposits latitude\n",
      "* 2 distance to the nearest trench point\n",
      "* 3 the index of trench point\n",
      "* 4 trench point longitude\n",
      "* 5 trench point latitude\n",
      "* 6 subducting convergence (relative to trench) velocity magnitude (in cm/yr)\n",
      "* 7 subducting convergence velocity obliquity angle (angle between trench normal vector and convergence velocity vector)\n",
      "* 8 trench absolute (relative to anchor plate) velocity magnitude (in cm/yr)\n",
      "* 9 trench absolute velocity obliquity angle (angle between trench normal vector and trench absolute velocity vector)\n",
      "* 10 length of arc segment (in degrees) that current point is on\n",
      "* 11 trench normal azimuth angle (clockwise starting at North, ie, 0 to 360 degrees) at current point\n",
      "* 12 subducting plate ID\n",
      "* 13 trench plate ID\n",
      "* 14 distance (in degrees) along the trench line to the nearest trench edge\n",
      "* 15 the distance (in degrees) along the trench line from the start edge of the trench\n",
      "* 16 convergence velocity orthogonal (in cm/yr)\n",
      "* 17 convergence velocity parallel (in cm/yr)\n",
      "* 18 the trench plate absolute velocity orthogonal (in cm/yr)\n",
      "* 19 the trench plate absolute velocity parallel (in cm/yr)\n",
      "* 20 the subducting plate absolute velocity magnitude (in cm/yr)\n",
      "* 21 the subducting plate absolute velocity obliquity angle (in degrees)\n",
      "* 22 the subducting plate absolute velocity orthogonal\n",
      "* 23 the subducting plate absolute velocity parallel\n",
      "* 24 sea floor age\n",
      "* 25 subduction volume(km3y)\n",
      "* 26 decompacted sediment thickness\n",
      "* 27 sediment thickness\n",
      "* 28 ocean crust carb percent\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from parameters_n1 import parameters \n",
    "import Utils_c1 as Utils\n",
    "\n",
    "#load data \n",
    "coreg_input_data = pd.read_csv('test-case-clennett/coreg_input/02_NA_Clennett_Positives_PlateID.csv')\n",
    "coreg_output_data = pd.read_csv('test-case-clennett/coreg_output/02_NA_Clennett_Positives_PlateID.csv')\n",
    "print('The shape of coregistration input data is: ', coreg_input_data.shape)\n",
    "print('The shape of coregistration output data is: ', coreg_output_data.shape)\n",
    "\n",
    "if coreg_input_data.shape[0] == coreg_output_data.shape[0]:\n",
    "    print('Good! The input and output data has the same length ', coreg_output_data.shape[0])\n",
    "\n",
    "print()\n",
    "print('the coregistration input data')\n",
    "display(coreg_input_data)\n",
    "print('the coregistration output data')\n",
    "display(coreg_output_data)\n",
    "print('the columns in coregistration output data are: ')\n",
    "Utils.print_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After having a look at the data, let's start selecting features and add labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lon', 'lat', 'age', 'plate_id', 'recon_lon', 'recon_lat', 'distance',\n",
      "       'sub_idx', 'trench_lon', 'trench_lat', 'conv_rate', 'conv_angle',\n",
      "       'trench_abs_rate', 'trench_abs_angle', 'arc_len', 'trench_norm',\n",
      "       'subducting_pid', 'trench_pid', 'dist_nearest_edge', 'dist_from_start',\n",
      "       'conv_ortho', 'conv_paral', 'trench_abs_ortho', 'trench_abs_paral',\n",
      "       'subducting_abs_rate', 'subducting_abs_angle', 'subducting_abs_ortho',\n",
      "       'subducting_abs_paral'],\n",
      "      dtype='object')\n",
      "\n",
      "generated files:\n",
      "test-case-clennett/ml_input/negative_c1_all_columns.csv\n",
      "test-case-clennett/ml_input/positive.csv\n",
      "test-case-clennett/ml_input/candidates_all_columns.csv\n",
      "test-case-clennett/ml_input/negative_c1.csv\n",
      "test-case-clennett/ml_input/candidates.csv\n",
      "test-case-clennett/ml_input/negative_c2.csv\n",
      "test-case-clennett/ml_input/positive_all_columns.csv\n",
      "test-case-clennett/ml_input/negative_c2_all_columns.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from parameters_n1 import parameters \n",
    "import Utils_c1 as Utils\n",
    "\n",
    "import os\n",
    "\n",
    "coreg_out_dir = Utils.get_coreg_output_dir()\n",
    "positive_data = pd.read_csv(coreg_out_dir + '/02_NA_Clennett_Positives_PlateID.csv')\n",
    "negative_data = pd.read_csv(coreg_out_dir + '/02_NA_Clennett_Negatives_1_PlateID.csv')\n",
    "candidates_data = pd.read_csv(coreg_out_dir + '/deposit_candidates.csv')\n",
    "\n",
    "print(positive_data.columns)\n",
    "\n",
    "feature_names = parameters['feature_names']\n",
    "\n",
    "positive_features = positive_data[feature_names].dropna()\n",
    "negative_features = negative_data[feature_names].dropna()\n",
    "candidates_features = candidates_data[feature_names].dropna()\n",
    "\n",
    "positive_features['label']=True\n",
    "negative_features['label']=False\n",
    "\n",
    "#save the data\n",
    "positive_features.to_csv(Utils.get_ml_input_dir() + 'positive.csv', index=False)\n",
    "negative_features.to_csv(Utils.get_ml_input_dir() + 'negative_c1.csv', index=False)\n",
    "candidates_features.to_csv(Utils.get_ml_input_dir() + 'candidates.csv', index=False)\n",
    "\n",
    "positive_data.iloc[positive_features.index].to_csv(Utils.get_ml_input_dir() + 'positive_all_columns.csv', index=False)\n",
    "negative_data.iloc[negative_features.index].to_csv(Utils.get_ml_input_dir() + 'negative_c1_all_columns.csv', index=False)\n",
    "candidates_data.iloc[candidates_features.index].to_csv(Utils.get_ml_input_dir() + '/candidates_all_columns.csv', index=False)\n",
    "\n",
    "import glob\n",
    "files = glob.glob(Utils.get_ml_input_dir() + '*')\n",
    "print('\\ngenerated files:')\n",
    "for f in files:\n",
    "    print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lon', 'lat', 'age', 'plate_id', 'recon_lon', 'recon_lat', 'distance',\n",
      "       'sub_idx', 'trench_lon', 'trench_lat', 'conv_rate', 'conv_angle',\n",
      "       'trench_abs_rate', 'trench_abs_angle', 'arc_len', 'trench_norm',\n",
      "       'subducting_pid', 'trench_pid', 'dist_nearest_edge', 'dist_from_start',\n",
      "       'conv_ortho', 'conv_paral', 'trench_abs_ortho', 'trench_abs_paral',\n",
      "       'subducting_abs_rate', 'subducting_abs_angle', 'subducting_abs_ortho',\n",
      "       'subducting_abs_paral'],\n",
      "      dtype='object')\n",
      "\n",
      "generated files:\n",
      "test-case-clennett/ml_input/negative_c1_all_columns.csv\n",
      "test-case-clennett/ml_input/positive.csv\n",
      "test-case-clennett/ml_input/candidates_all_columns.csv\n",
      "test-case-clennett/ml_input/negative_c1.csv\n",
      "test-case-clennett/ml_input/candidates.csv\n",
      "test-case-clennett/ml_input/negative_c2.csv\n",
      "test-case-clennett/ml_input/positive_all_columns.csv\n",
      "test-case-clennett/ml_input/negative_c2_all_columns.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from parameters_n2 import parameters \n",
    "import Utils_c2 as Utils\n",
    "\n",
    "import os\n",
    "\n",
    "coreg_out_dir = Utils.get_coreg_output_dir()\n",
    "positive_data = pd.read_csv(coreg_out_dir + '/02_NA_Clennett_Positives_PlateID.csv')\n",
    "negative_data = pd.read_csv(coreg_out_dir + '/02_NA_Clennett_Negatives_2_PlateID.csv')\n",
    "candidates_data = pd.read_csv(coreg_out_dir + '/deposit_candidates.csv')\n",
    "\n",
    "print(positive_data.columns)\n",
    "\n",
    "feature_names = parameters['feature_names']\n",
    "\n",
    "positive_features = positive_data[feature_names].dropna()\n",
    "negative_features = negative_data[feature_names].dropna()\n",
    "candidates_features = candidates_data[feature_names].dropna()\n",
    "\n",
    "positive_features['label']=True\n",
    "negative_features['label']=False\n",
    "\n",
    "#save the data\n",
    "positive_features.to_csv(Utils.get_ml_input_dir() + 'positive.csv', index=False)\n",
    "negative_features.to_csv(Utils.get_ml_input_dir() + 'negative_c2.csv', index=False)\n",
    "candidates_features.to_csv(Utils.get_ml_input_dir() + 'candidates.csv', index=False)\n",
    "\n",
    "positive_data.iloc[positive_features.index].to_csv(Utils.get_ml_input_dir() + 'positive_all_columns.csv', index=False)\n",
    "negative_data.iloc[negative_features.index].to_csv(Utils.get_ml_input_dir() + 'negative_c2_all_columns.csv', index=False)\n",
    "candidates_data.iloc[candidates_features.index].to_csv(Utils.get_ml_input_dir() + '/candidates_all_columns.csv', index=False)\n",
    "\n",
    "import glob\n",
    "files = glob.glob(Utils.get_ml_input_dir() + '*')\n",
    "print('\\ngenerated files:')\n",
    "for f in files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lon', 'lat', 'age', 'plate_id', 'recon_lon', 'recon_lat', 'distance',\n",
      "       'sub_idx', 'trench_lon', 'trench_lat', 'conv_rate', 'conv_angle',\n",
      "       'trench_abs_rate', 'trench_abs_angle', 'arc_len', 'trench_norm',\n",
      "       'subducting_pid', 'trench_pid', 'dist_nearest_edge', 'dist_from_start',\n",
      "       'conv_ortho', 'conv_paral', 'trench_abs_ortho', 'trench_abs_paral',\n",
      "       'subducting_abs_rate', 'subducting_abs_angle', 'subducting_abs_ortho',\n",
      "       'subducting_abs_paral'],\n",
      "      dtype='object')\n",
      "\n",
      "generated files:\n",
      "test-case-clennett/ml_input/negative_c1_all_columns.csv\n",
      "test-case-clennett/ml_input/positive.csv\n",
      "test-case-clennett/ml_input/candidates_all_columns.csv\n",
      "test-case-clennett/ml_input/negative_c1.csv\n",
      "test-case-clennett/ml_input/candidates.csv\n",
      "test-case-clennett/ml_input/negative_c2.csv\n",
      "test-case-clennett/ml_input/positive_all_columns.csv\n",
      "test-case-clennett/ml_input/negative_c3.csv\n",
      "test-case-clennett/ml_input/negative_c2_all_columns.csv\n",
      "test-case-clennett/ml_input/negative_c3_all_columns.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from parameters_n3 import parameters \n",
    "import Utils_c3 as Utils\n",
    "\n",
    "import os\n",
    "\n",
    "coreg_out_dir = Utils.get_coreg_output_dir()\n",
    "positive_data = pd.read_csv(coreg_out_dir + '/02_NA_Clennett_Positives_PlateID.csv')\n",
    "negative_data = pd.read_csv(coreg_out_dir + '/02_NA_Clennett_Negatives_3_PlateID.csv')\n",
    "candidates_data = pd.read_csv(coreg_out_dir + '/deposit_candidates.csv')\n",
    "\n",
    "print(positive_data.columns)\n",
    "\n",
    "feature_names = parameters['feature_names']\n",
    "\n",
    "positive_features = positive_data[feature_names].dropna()\n",
    "negative_features = negative_data[feature_names].dropna()\n",
    "candidates_features = candidates_data[feature_names].dropna()\n",
    "\n",
    "positive_features['label']=True\n",
    "negative_features['label']=False\n",
    "\n",
    "#save the data\n",
    "positive_features.to_csv(Utils.get_ml_input_dir() + 'positive.csv', index=False)\n",
    "negative_features.to_csv(Utils.get_ml_input_dir() + 'negative_c3.csv', index=False)\n",
    "candidates_features.to_csv(Utils.get_ml_input_dir() + 'candidates.csv', index=False)\n",
    "\n",
    "positive_data.iloc[positive_features.index].to_csv(Utils.get_ml_input_dir() + 'positive_all_columns.csv', index=False)\n",
    "negative_data.iloc[negative_features.index].to_csv(Utils.get_ml_input_dir() + 'negative_c3_all_columns.csv', index=False)\n",
    "candidates_data.iloc[candidates_features.index].to_csv(Utils.get_ml_input_dir() + '/candidates_all_columns.csv', index=False)\n",
    "\n",
    "import glob\n",
    "files = glob.glob(Utils.get_ml_input_dir() + '*')\n",
    "print('\\ngenerated files:')\n",
    "for f in files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lon', 'lat', 'age', 'plate_id', 'recon_lon', 'recon_lat', 'distance',\n",
      "       'sub_idx', 'trench_lon', 'trench_lat', 'conv_rate', 'conv_angle',\n",
      "       'trench_abs_rate', 'trench_abs_angle', 'arc_len', 'trench_norm',\n",
      "       'subducting_pid', 'trench_pid', 'dist_nearest_edge', 'dist_from_start',\n",
      "       'conv_ortho', 'conv_paral', 'trench_abs_ortho', 'trench_abs_paral',\n",
      "       'subducting_abs_rate', 'subducting_abs_angle', 'subducting_abs_ortho',\n",
      "       'subducting_abs_paral'],\n",
      "      dtype='object')\n",
      "\n",
      "generated files:\n",
      "test-case-clennett/ml_input/negative_c1_all_columns.csv\n",
      "test-case-clennett/ml_input/positive.csv\n",
      "test-case-clennett/ml_input/candidates_all_columns.csv\n",
      "test-case-clennett/ml_input/negative_c1.csv\n",
      "test-case-clennett/ml_input/candidates.csv\n",
      "test-case-clennett/ml_input/negative_c2.csv\n",
      "test-case-clennett/ml_input/positive_all_columns.csv\n",
      "test-case-clennett/ml_input/negative_c3.csv\n",
      "test-case-clennett/ml_input/negative_c2_all_columns.csv\n",
      "test-case-clennett/ml_input/negative_c4.csv\n",
      "test-case-clennett/ml_input/negative_c3_all_columns.csv\n",
      "test-case-clennett/ml_input/negative_c4_all_columns.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from parameters_n4 import parameters \n",
    "import Utils_c4 as Utils\n",
    "\n",
    "import os\n",
    "\n",
    "coreg_out_dir = Utils.get_coreg_output_dir()\n",
    "positive_data = pd.read_csv(coreg_out_dir + '/02_NA_Clennett_Positives_PlateID.csv')\n",
    "negative_data = pd.read_csv(coreg_out_dir + '/02_NA_Clennett_Negatives_4_PlateID.csv')\n",
    "candidates_data = pd.read_csv(coreg_out_dir + '/deposit_candidates.csv')\n",
    "\n",
    "print(positive_data.columns)\n",
    "\n",
    "feature_names = parameters['feature_names']\n",
    "\n",
    "positive_features = positive_data[feature_names].dropna()\n",
    "negative_features = negative_data[feature_names].dropna()\n",
    "candidates_features = candidates_data[feature_names].dropna()\n",
    "\n",
    "positive_features['label']=True\n",
    "negative_features['label']=False\n",
    "\n",
    "#save the data\n",
    "positive_features.to_csv(Utils.get_ml_input_dir() + 'positive.csv', index=False)\n",
    "negative_features.to_csv(Utils.get_ml_input_dir() + 'negative_c4.csv', index=False)\n",
    "candidates_features.to_csv(Utils.get_ml_input_dir() + 'candidates.csv', index=False)\n",
    "\n",
    "positive_data.iloc[positive_features.index].to_csv(Utils.get_ml_input_dir() + 'positive_all_columns.csv', index=False)\n",
    "negative_data.iloc[negative_features.index].to_csv(Utils.get_ml_input_dir() + 'negative_c4_all_columns.csv', index=False)\n",
    "candidates_data.iloc[candidates_features.index].to_csv(Utils.get_ml_input_dir() + '/candidates_all_columns.csv', index=False)\n",
    "\n",
    "import glob\n",
    "files = glob.glob(Utils.get_ml_input_dir() + '*')\n",
    "print('\\ngenerated files:')\n",
    "for f in files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lon', 'lat', 'age', 'plate_id', 'recon_lon', 'recon_lat', 'distance',\n",
      "       'sub_idx', 'trench_lon', 'trench_lat', 'conv_rate', 'conv_angle',\n",
      "       'trench_abs_rate', 'trench_abs_angle', 'arc_len', 'trench_norm',\n",
      "       'subducting_pid', 'trench_pid', 'dist_nearest_edge', 'dist_from_start',\n",
      "       'conv_ortho', 'conv_paral', 'trench_abs_ortho', 'trench_abs_paral',\n",
      "       'subducting_abs_rate', 'subducting_abs_angle', 'subducting_abs_ortho',\n",
      "       'subducting_abs_paral'],\n",
      "      dtype='object')\n",
      "\n",
      "generated files:\n",
      "test-case-clennett/ml_input/negative_c1_all_columns.csv\n",
      "test-case-clennett/ml_input/positive.csv\n",
      "test-case-clennett/ml_input/candidates_all_columns.csv\n",
      "test-case-clennett/ml_input/negative_c1.csv\n",
      "test-case-clennett/ml_input/negative_c_all_columns.csv\n",
      "test-case-clennett/ml_input/candidates.csv\n",
      "test-case-clennett/ml_input/negative_c2.csv\n",
      "test-case-clennett/ml_input/positive_all_columns.csv\n",
      "test-case-clennett/ml_input/negative_c5_all_columns.csv\n",
      "test-case-clennett/ml_input/negative_c3.csv\n",
      "test-case-clennett/ml_input/negative_c2_all_columns.csv\n",
      "test-case-clennett/ml_input/negative_c4.csv\n",
      "test-case-clennett/ml_input/negative_c5.csv\n",
      "test-case-clennett/ml_input/negative_c3_all_columns.csv\n",
      "test-case-clennett/ml_input/negative_c4_all_columns.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from parameters_n5 import parameters \n",
    "import Utils_c5 as Utils\n",
    "\n",
    "import os\n",
    "\n",
    "coreg_out_dir = Utils.get_coreg_output_dir()\n",
    "positive_data = pd.read_csv(coreg_out_dir + '/02_NA_Clennett_Positives_PlateID.csv')\n",
    "negative_data = pd.read_csv(coreg_out_dir + '/02_NA_Clennett_Negatives_5_PlateID.csv')\n",
    "candidates_data = pd.read_csv(coreg_out_dir + '/deposit_candidates.csv')\n",
    "\n",
    "print(positive_data.columns)\n",
    "\n",
    "feature_names = parameters['feature_names']\n",
    "\n",
    "positive_features = positive_data[feature_names].dropna()\n",
    "negative_features = negative_data[feature_names].dropna()\n",
    "candidates_features = candidates_data[feature_names].dropna()\n",
    "\n",
    "positive_features['label']=True\n",
    "negative_features['label']=False\n",
    "\n",
    "#save the data\n",
    "positive_features.to_csv(Utils.get_ml_input_dir() + 'positive.csv', index=False)\n",
    "negative_features.to_csv(Utils.get_ml_input_dir() + 'negative_c5.csv', index=False)\n",
    "candidates_features.to_csv(Utils.get_ml_input_dir() + 'candidates.csv', index=False)\n",
    "\n",
    "positive_data.iloc[positive_features.index].to_csv(Utils.get_ml_input_dir() + 'positive_all_columns.csv', index=False)\n",
    "negative_data.iloc[negative_features.index].to_csv(Utils.get_ml_input_dir() + 'negative_c5_all_columns.csv', index=False)\n",
    "candidates_data.iloc[candidates_features.index].to_csv(Utils.get_ml_input_dir() + '/candidates_all_columns.csv', index=False)\n",
    "\n",
    "import glob\n",
    "files = glob.glob(Utils.get_ml_input_dir() + '*')\n",
    "print('\\ngenerated files:')\n",
    "for f in files:\n",
    "    print(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
